# TRM config for DSL token-based training
# Vocabulary: ~442 DSL tokens (math, logic, ARC primitives)
# Treats DSL program generation as constraint satisfaction

name: recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1
loss:
  name: losses@ACTLossHead
  loss_type: stablemax_cross_entropy

# ACT (Adaptive Computation Time) settings
halt_exploration_prob: 0.1
halt_max_steps: 8  # Fewer steps than ARC since DSL sequences are shorter

# Recursive reasoning cycles
# H_cycles: outer refinement iterations
# L_cycles: inner reasoning iterations per H step
H_cycles: 3
L_cycles: 4

# Transformer layers
H_layers: 0  # Ignored in current implementation
L_layers: 2  # Number of transformer/MLP blocks in L-level

# Model dimensions
hidden_size: 256  # Smaller than ARC since DSL vocab is more structured
num_heads: 4
expansion: 4

# Puzzle embeddings - one per module (algebra, arithmetic, etc.)
puzzle_emb_ndim: ${.hidden_size}
puzzle_emb_len: 8  # Shorter since DSL has explicit structure

# Position encodings
pos_encodings: rope  # Rotary position embeddings

# Precision
forward_dtype: bfloat16

# Architecture options
mlp_t: False  # Use attention (not pure MLP) for L-level
no_ACT_continue: True  # Simplified ACT halting logic

