# TRM config for DSL token-based training
# Vocabulary: ~442 DSL tokens (math, logic, ARC primitives)
# Treats DSL program generation as constraint satisfaction

name: recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1
loss:
  name: losses@ACTLossHead
  loss_type: stablemax_cross_entropy

# ACT (Adaptive Computation Time) settings
halt_exploration_prob: 0.1
halt_max_steps: 16  # Fewer steps than ARC since DSL sequences are shorter

# Recursive reasoning cycles
# H_cycles: outer refinement iterations
# L_cycles: inner reasoning iterations per H step
H_cycles: 3
L_cycles: 4  # Increased for deeper reasoning

# Transformer layers
H_layers: 0  # Ignored in current implementation
L_layers: 2  # Number of transformer/MLP blocks in L-level

# Model dimensions
hidden_size: 256  # Smaller model (~1.9M params)
num_heads: 8  # 256 / 8 = 32 dim per head
expansion: 4

# Puzzle embeddings - one per module (algebra, arithmetic, etc.)
puzzle_emb_ndim: ${.hidden_size}
puzzle_emb_len: 8  # 4 modules, DSL is self-describing

# Position encodings
pos_encodings: rope  # Rotary position embeddings

# Precision
forward_dtype: bfloat16

# Architecture options
mlp_t: False  # Use attention (not pure MLP) for L-level
no_ACT_continue: True  # Simplified ACT halting logic

