# English→DSL TRM training config
# Train TRM to solve the "puzzle": given English chars, produce DSL tokens

defaults:
  - arch: trm_dsl
  - _self_

hydra:
  output_subdir: null

# Data path - English→DSL dataset built by dataset/build_english_trm_dataset.py
data_paths: ['data/english_to_dsl_trm']
data_paths_test: []

# No evaluator for now
evaluators: []

# Hyperparams - Training
# Small batch size since dataset is tiny (10 examples)
global_batch_size: 10

# Many epochs to overfit on small data (proof of concept)
epochs: 1000
eval_interval: 100
checkpoint_every_eval: True

# Learning rate
lr: 1e-3
lr_min_ratio: 0.1
lr_warmup_steps: 50

# Standard hyperparameter settings
beta1: 0.9
beta2: 0.95
weight_decay: 0.1
puzzle_emb_weight_decay: 0.1

# Hyperparams - Puzzle embeddings
puzzle_emb_lr: 1e-2

seed: 42
min_eval_interval: 0

# EMA for stable evaluation
ema: True
ema_rate: 0.999

freeze_weights: False

