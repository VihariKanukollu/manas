# DSL training config for TRM
# Train on math DSL token sequences

defaults:
  - arch: trm_dsl
  - _self_

hydra:
  output_subdir: null

# Data path - DSL dataset built by dataset/build_dsl_dataset.py
data_paths: ['data/dsl_trm']
data_paths_test: []

# No evaluator for now (can add DSL-specific evaluator later)
evaluators: []

# Hyperparams - Training
# Smaller batch size since DSL dataset is smaller than ARC
global_batch_size: 768

epochs: 100
eval_interval: 10
checkpoint_every_eval: True

# Learning rate
lr: 1e-4
lr_min_ratio: 0.1
lr_warmup_steps: 2000

# Standard hyperparameter settings for LM
beta1: 0.9
beta2: 0.95
weight_decay: 0.1
puzzle_emb_weight_decay: 0.1

# Hyperparams - Puzzle embeddings (per-module embeddings)
puzzle_emb_lr: 1e-2

seed: 42
min_eval_interval: 0

# EMA (Exponential Moving Average) for stable evaluation
ema: True
ema_rate: 0.999

freeze_weights: False

