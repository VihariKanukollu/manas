"""
DSL token-based dataset loader.

Loads JSONL files generated by `dev.gen_dsl_tokens`:

    data/dsl_tokens_{small,large}/
        train.jsonl
        test_id.jsonl
        test_ood.jsonl

Each line is a JSON object with:
    - "token_ids": list of GyanDSLToken integer IDs
    - "question": original question text
    - "answer": ground truth answer

This is a pure DSL token dataset - no BPE, no text tokenization.
The model learns to generate DSL programs directly.
"""

import os
import json
import random
from typing import List, Dict, Any, Optional

import torch

from model.common import get_dist_info
from dsl.tokens import GyanDSLToken, get_vocab_size


def load_dsl_jsonl(filepath: str) -> List[Dict[str, Any]]:
    """Load a JSONL file into a list of dicts."""
    examples = []
    with open(filepath, "r") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            examples.append(json.loads(line))
    return examples


def get_dsl_data_dir(root_name: str = "data/dsl_tokens_small") -> str:
    """
    Returns the directory where DSL data lives.

    By default this is a relative path inside the repo:
        data/dsl_tokens_small
    """
    if os.path.isabs(root_name):
        return root_name

    repo_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    data_dir = os.path.join(repo_dir, root_name)
    return data_dir


class DSLTokenDataset:
    """
    Dataset for DSL token-based training.

    Returns (input_ids, target_ids) pairs for next-token prediction.
    No BPE - tokens are GyanDSLToken IDs directly.
    """

    def __init__(
        self,
        split: str,
        max_seq_len: int = 128,
        root_name: str = "data/dsl_tokens_small",
    ):
        assert split in ["train", "test_id", "test_ood"], "split must be train|test_id|test_ood"
        self.split = split
        self.max_seq_len = max_seq_len

        data_dir = get_dsl_data_dir(root_name)
        filepath = os.path.join(data_dir, f"{split}.jsonl")
        assert os.path.exists(filepath), f"DSL data not found at {filepath}"
        self.examples = load_dsl_jsonl(filepath)

        self.pad_token_id = GyanDSLToken.PAD.value
        self.bos_token_id = GyanDSLToken.BOS.value
        self.eos_token_id = GyanDSLToken.EOS.value

    def __len__(self) -> int:
        return len(self.examples)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """
        Returns the token IDs and raw example.

        The token_ids already include BOS and EOS from generation.
        """
        example = self.examples[idx]
        token_ids = example["token_ids"]

        # Truncate if needed
        if len(token_ids) > self.max_seq_len:
            token_ids = token_ids[:self.max_seq_len]

        return {
            "token_ids": token_ids,
            "example": example,
        }

    def get_example(self, idx: int) -> Dict[str, Any]:
        """Return the raw example dict."""
        return self.examples[idx]


def dsl_token_collate_fn(
    batch: List[Dict[str, Any]],
    pad_token_id: int,
    max_seq_len: int,
) -> Dict[str, torch.Tensor]:
    """
    Collate function for padding sequences and building batch tensors.

    Returns:
        - input_ids: [B, T-1] - input tokens (everything except last)
        - targets: [B, T-1] - target tokens (shifted by 1)
        - attention_mask: [B, T-1] - True for real tokens
    """
    batch_size = len(batch)
    seqs = [item["token_ids"] for item in batch]

    # Truncate to max_seq_len
    seqs = [s[:max_seq_len] for s in seqs]
    lengths = [len(s) for s in seqs]
    max_len = max(lengths)

    # For next-token prediction: input is [:-1], target is [1:]
    # So we need sequences of length max_len, then split
    padded = torch.full((batch_size, max_len), pad_token_id, dtype=torch.long)
    for i, s in enumerate(seqs):
        padded[i, :len(s)] = torch.tensor(s, dtype=torch.long)

    # Input: all but last token
    input_ids = padded[:, :-1]
    # Target: all but first token
    targets = padded[:, 1:]

    # Attention mask: True where we have real tokens (not PAD)
    # For input_ids, mask is based on original lengths - 1
    attention_mask = torch.zeros_like(input_ids, dtype=torch.bool)
    for i, length in enumerate(lengths):
        attention_mask[i, :length-1] = True

    return {
        "input_ids": input_ids,
        "targets": targets,
        "attention_mask": attention_mask,
    }


def dsl_token_data_loader(
    split: str,
    batch_size: int,
    max_seq_len: int = 128,
    root_name: str = "data/dsl_tokens_small",
    shuffle: bool = True,
    num_workers: int = 0,
):
    """
    Create a DataLoader for DSL token training.

    This is a simple non-distributed loader. For distributed training,
    use the distributed version below.
    """
    dataset = DSLTokenDataset(split, max_seq_len, root_name)
    pad_token_id = dataset.pad_token_id

    def collate_fn(batch):
        return dsl_token_collate_fn(batch, pad_token_id, max_seq_len)

    loader = torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
        drop_last=True,
    )
    return loader


def dsl_token_distributed_data_loader(
    split: str,
    batch_size: int,
    max_seq_len: int = 128,
    root_name: str = "data/dsl_tokens_small",
    shuffle: bool = True,
    num_workers: int = 0,
):
    """
    Create a distributed DataLoader for DSL token training.

    Each rank gets a different subset of the data.
    """
    dataset = DSLTokenDataset(split, max_seq_len, root_name)
    pad_token_id = dataset.pad_token_id

    # get_dist_info() returns (ddp, rank, local_rank, world_size)
    ddp, rank, local_rank, world_size = get_dist_info()

    sampler = torch.utils.data.distributed.DistributedSampler(
        dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=shuffle,
    )

    def collate_fn(batch):
        return dsl_token_collate_fn(batch, pad_token_id, max_seq_len)

    loader = torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
        drop_last=True,
    )
    return loader


# ---------------------------------------------------------------------------
# Quick test
# ---------------------------------------------------------------------------

if __name__ == "__main__":
    print(f"Vocab size: {get_vocab_size()}")

    # Test loading
    dataset = DSLTokenDataset("train", max_seq_len=128, root_name="data/dsl_tokens_small")
    print(f"Train examples: {len(dataset)}")

    # Test a batch
    loader = dsl_token_data_loader("train", batch_size=4, max_seq_len=128, root_name="data/dsl_tokens_small")
    batch = next(iter(loader))
    print(f"Input shape: {batch['input_ids'].shape}")
    print(f"Target shape: {batch['targets'].shape}")
    print(f"Mask shape: {batch['attention_mask'].shape}")
    print()
    print("First example input_ids:", batch['input_ids'][0].tolist())
    print("First example targets:", batch['targets'][0].tolist())

