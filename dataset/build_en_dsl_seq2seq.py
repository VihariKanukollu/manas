#!/usr/bin/env python
"""
Build EN-DSL seq2seq training JSONL files from DeepMind-style data.

Source examples are the JSONL files produced by `dev/gen_full_math.py` (or any
compatible generator) that already contain `en_token_names` alongside the usual
DSL tokens.

Each output line has the shape:

    {
        "id": ...,
        "module": "numbers__gcd",
        "question": "Calculate the greatest common factor of 6 and 426.",
        "en_tokens": ["BOS", "EN_QUERY", "EN_Q_ATTR", ...]
    }

You can adjust `SRC_DIR`, `DST_DIR`, and `SPLITS` as needed.
"""

import json
from pathlib import Path
from typing import Dict, Any, Iterable, Tuple, List


# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

# Default: point to a DeepMind+EN dataset generated by `gen_full_math.py`.
# You will typically run something like:
#   python dev/gen_full_math.py --output_dir data/deepmind_en_dsl_raw ...
# and then set SRC_DIR to that output directory.
#
# For existing EN-annotated datasets in this repo, you can point SRC_DIR at
# any folder that has `train.jsonl`, `test_id.jsonl`, and `test_ood.jsonl`
# with `en_token_names` fields. For example:
#   SRC_DIR = Path("data/math_dsl_comp_en_sem_small")
#   SRC_DIR = Path("data/test_gsm8k_en")
# Default here uses the GSM8K+EN dataset already checked into the repo.
SRC_DIR = Path("data/test_gsm8k_en")

# Where to write the seq2seq-style EN-DSL dataset
DST_DIR = Path("data/en_dsl_seq2seq")

# Mapping from destination split name -> source split name.
# `gen_full_math.py` uses "train", "test_id", "test_ood". We map those into
# the more conventional "train", "valid", "test" names for seq2seq training.
SPLITS: List[Tuple[str, str]] = [
    ("train", "train"),
    ("valid", "test_id"),
    ("test", "test_ood"),
]


def iter_jsonl(path: Path) -> Iterable[Dict[str, Any]]:
    """Yield JSON objects from a .jsonl file."""
    with path.open() as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            yield json.loads(line)


def convert_split(dst_split: str, src_split: str) -> None:
    """
    Convert one split from the source DeepMind+EN dataset into seq2seq format.

    We keep only examples that already have `en_token_names`.
    """
    src_file = SRC_DIR / f"{src_split}.jsonl"
    dst_file = DST_DIR / f"{dst_split}.jsonl"

    if not src_file.exists():
        raise FileNotFoundError(f"Source split file not found: {src_file}")

    dst_file.parent.mkdir(parents=True, exist_ok=True)

    n = 0
    with dst_file.open("w") as out:
        for ex in iter_jsonl(src_file):
            question = ex.get("question")
            module = ex.get("module")
            en_token_names = ex.get("en_token_names")

            # Skip if EN-DSL semantics are missing
            if not question or not en_token_names:
                continue

            record = {
                "id": ex.get("id", n),
                "module": module,
                "question": question,
                "en_tokens": en_token_names,
            }
            out.write(json.dumps(record) + "\n")
            n += 1

    print(f"[{dst_split}] wrote {n} examples to {dst_file}")


def main() -> None:
    for dst_split, src_split in SPLITS:
        convert_split(dst_split, src_split)


if __name__ == "__main__":
    main()


